{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/nlp/rajak/flask/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os \n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms as transforms\n",
    "# import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from PIL import Image\n",
    "sys.path.append('..')  \n",
    "import scipy\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from model.model import MMC  \n",
    "from src.config import Config\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    name='MMC',\n",
    "    dataset='Food101',  # Adjust as per your dataset or requirement\n",
    "    text_type='abstract',\n",
    "    mmc='UniSMMC',\n",
    "    mmc_tao=0.07,\n",
    "    batch_size=32,\n",
    "    lr_mm=0.001,\n",
    "    min_epoch=1,\n",
    "    valid_step=50,\n",
    "    max_length=512,\n",
    "    text_encoder='bert_base',\n",
    "    image_encoder='vit_base',\n",
    "    text_out=768,\n",
    "    img_out=768,\n",
    "    lr_mm_cls=0.001,\n",
    "    mm_dropout=0.0,\n",
    "    lr_text_tfm=2e-5,\n",
    "    lr_img_tfm=5e-5,\n",
    "    lr_img_cls=0.0001,\n",
    "    lr_text_cls=5e-5,\n",
    "    text_dropout=0.0,\n",
    "    img_dropout=0.1,\n",
    "    nplot='',\n",
    "    data_dir='../datasets/',  # Ensure this path is correct in your notebook environment\n",
    "    test_only=False,\n",
    "    pretrained_dir='../pretrained_models',  # Adjust as necessary\n",
    "    model_save_dir='Path/To/results/models',\n",
    "    res_save_dir='Path/To/results/results',\n",
    "    fig_save_dir='Path/To/results/imgs',\n",
    "    logs_dir='Path/To/results/logs',\n",
    "    local_rank=-1,\n",
    "    seeds=None,\n",
    "    model_path='./Path/To/results/models',\n",
    "    save_model=True,\n",
    "    cross_attention=False,\n",
    "    text_mixup=False,\n",
    "    image_mixup=False,\n",
    "    image_embedding_mixup=False,\n",
    "    alpha=0.2,\n",
    "    multi_mixup=True,\n",
    "    mixup_pct=0.33,\n",
    "    lambda_mixup=0.1,\n",
    "    mixup_beta=0.15,\n",
    "    mixup_s_thresh=0.5,\n",
    "    lr_scheduler='ReduceLROnPlateau',\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "config = Config(args)\n",
    "args = config.get_config()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if args.local_rank == -1:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "args.device = device\n",
    "print(args.data_dir)\n",
    "args.data_dir = os.path.join(args.data_dir, args.dataset)\n",
    "\n",
    "#args.best_model_save_path = os.path.join(args.model_save_dir, f'{args.dataset}-best-{time.strftime(\"%Y%m%d-%H%M%S\")}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/nlp/rajak/flask/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/nlp/rajak/flask/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module BertEmbeddings is treated as a zero-op.\n",
      "Warning: module BertSdpaSelfAttention is treated as a zero-op.\n",
      "Warning: module BertSelfOutput is treated as a zero-op.\n",
      "Warning: module BertAttention is treated as a zero-op.\n",
      "Warning: module GELUActivation is treated as a zero-op.\n",
      "Warning: module BertIntermediate is treated as a zero-op.\n",
      "Warning: module BertOutput is treated as a zero-op.\n",
      "Warning: module BertLayer is treated as a zero-op.\n",
      "Warning: module BertEncoder is treated as a zero-op.\n",
      "Warning: module Tanh is treated as a zero-op.\n",
      "Warning: module BertPooler is treated as a zero-op.\n",
      "Warning: module BertModel is treated as a zero-op.\n",
      "Warning: module TextEncoder is treated as a zero-op.\n",
      "Warning: module ViTPatchEmbeddings is treated as a zero-op.\n",
      "Warning: module ViTEmbeddings is treated as a zero-op.\n",
      "Warning: module ViTSdpaSelfAttention is treated as a zero-op.\n",
      "Warning: module ViTSelfOutput is treated as a zero-op.\n",
      "Warning: module ViTSdpaAttention is treated as a zero-op.\n",
      "Warning: module ViTIntermediate is treated as a zero-op.\n",
      "Warning: module ViTOutput is treated as a zero-op.\n",
      "Warning: module ViTLayer is treated as a zero-op.\n",
      "Warning: module ViTEncoder is treated as a zero-op.\n",
      "Warning: module ViTPooler is treated as a zero-op.\n",
      "Warning: module ViTModel is treated as a zero-op.\n",
      "Warning: module ImageEncoder is treated as a zero-op.\n",
      "Warning: module MMC is treated as a zero-op.\n",
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "MMC(\n",
      "  171.89 M, 87.754% Params, 19.0 GMac, 100.000% MACs, \n",
      "  (text_encoder): TextEncoder(\n",
      "    85.65 M, 43.725% Params, 2.13 GMac, 11.195% MACs, \n",
      "    (text_encoder): BertModel(\n",
      "      85.65 M, 43.725% Params, 2.13 GMac, 11.195% MACs, \n",
      "      (embeddings): BertEmbeddings(\n",
      "        1.54 k, 0.001% Params, 19.2 KMac, 0.000% MACs, \n",
      "        (word_embeddings): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 512, 768)\n",
      "        (token_type_embeddings): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 2, 768)\n",
      "        (LayerNorm): LayerNorm(1.54 k, 0.001% Params, 19.2 KMac, 0.000% MACs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        85.05 M, 43.423% Params, 2.13 GMac, 11.192% MACs, \n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            7.09 M, 3.619% Params, 177.16 MMac, 0.933% MACs, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 1.207% Params, 59.08 MMac, 0.311% MACs, \n",
      "              (self): BertSdpaSelfAttention(\n",
      "                1.77 M, 0.905% Params, 44.29 MMac, 0.233% MACs, \n",
      "                (query): Linear(590.59 k, 0.302% Params, 14.76 MMac, 0.078% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.302% Params, 14.76 MMac, 0.078% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.302% Params, 14.76 MMac, 0.078% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.302% Params, 14.78 MMac, 0.078% MACs, \n",
      "                (dense): Linear(590.59 k, 0.302% Params, 14.76 MMac, 0.078% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.001% Params, 19.2 KMac, 0.000% MACs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              2.36 M, 1.206% Params, 59.06 MMac, 0.311% MACs, \n",
      "              (dense): Linear(2.36 M, 1.206% Params, 59.06 MMac, 0.311% MACs, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              2.36 M, 1.206% Params, 59.02 MMac, 0.311% MACs, \n",
      "              (dense): Linear(2.36 M, 1.205% Params, 59.0 MMac, 0.311% MACs, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.001% Params, 19.2 KMac, 0.000% MACs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        590.59 k, 0.302% Params, 590.59 KMac, 0.003% MACs, \n",
      "        (dense): Linear(590.59 k, 0.302% Params, 590.59 KMac, 0.003% MACs, in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (image_encoder): ImageEncoder(\n",
      "    86.24 M, 44.027% Params, 16.87 GMac, 88.805% MACs, \n",
      "    (image_encoder): ViTModel(\n",
      "      86.24 M, 44.027% Params, 16.87 GMac, 88.805% MACs, \n",
      "      (embeddings): ViTEmbeddings(\n",
      "        590.59 k, 0.302% Params, 115.76 MMac, 0.609% MACs, \n",
      "        (patch_embeddings): ViTPatchEmbeddings(\n",
      "          590.59 k, 0.302% Params, 115.76 MMac, 0.609% MACs, \n",
      "          (projection): Conv2d(590.59 k, 0.302% Params, 115.76 MMac, 0.609% MACs, 3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "        )\n",
      "        (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): ViTEncoder(\n",
      "        85.05 M, 43.423% Params, 16.75 GMac, 88.192% MACs, \n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x ViTLayer(\n",
      "            7.09 M, 3.619% Params, 1.4 GMac, 7.349% MACs, \n",
      "            (attention): ViTSdpaAttention(\n",
      "              2.36 M, 1.206% Params, 465.39 MMac, 2.450% MACs, \n",
      "              (attention): ViTSdpaSelfAttention(\n",
      "                1.77 M, 0.905% Params, 349.04 MMac, 1.838% MACs, \n",
      "                (query): Linear(590.59 k, 0.302% Params, 116.35 MMac, 0.613% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.302% Params, 116.35 MMac, 0.613% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.302% Params, 116.35 MMac, 0.613% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): ViTSelfOutput(\n",
      "                590.59 k, 0.302% Params, 116.35 MMac, 0.613% MACs, \n",
      "                (dense): Linear(590.59 k, 0.302% Params, 116.35 MMac, 0.613% MACs, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): ViTIntermediate(\n",
      "              2.36 M, 1.206% Params, 465.39 MMac, 2.450% MACs, \n",
      "              (dense): Linear(2.36 M, 1.206% Params, 465.39 MMac, 2.450% MACs, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            )\n",
      "            (output): ViTOutput(\n",
      "              2.36 M, 1.205% Params, 464.93 MMac, 2.448% MACs, \n",
      "              (dense): Linear(2.36 M, 1.205% Params, 464.93 MMac, 2.448% MACs, in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "            )\n",
      "            (layernorm_before): LayerNorm(1.54 k, 0.001% Params, 151.3 KMac, 0.001% MACs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "            (layernorm_after): LayerNorm(1.54 k, 0.001% Params, 151.3 KMac, 0.001% MACs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layernorm): LayerNorm(1.54 k, 0.001% Params, 151.3 KMac, 0.001% MACs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "      (pooler): ViTPooler(\n",
      "        590.59 k, 0.302% Params, 590.59 KMac, 0.003% MACs, \n",
      "        (dense): Linear(590.59 k, 0.302% Params, 590.59 KMac, 0.003% MACs, in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(3.07 k, 0.002% Params, 3.07 KMac, 0.000% MACs, in_features=1536, out_features=2, bias=True)\n",
      ")\n",
      "FLOPs: 19.0 GMac\n",
      "Params: 195.87 M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ptflops import get_model_complexity_info\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModel\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_ids, token_type_ids, attention_mask = inputs\n",
    "        outputs = self.text_encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        self.image_encoder = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        return self.feature_extractor\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.image_encoder(pixel_values=pixel_values)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "class MMC(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MMC, self).__init__()\n",
    "        self.text_encoder = TextEncoder(args.text_model_name)\n",
    "        self.image_encoder = ImageEncoder(args.image_model_name)\n",
    "        self.classifier = nn.Linear(self.text_encoder.text_encoder.config.hidden_size + \n",
    "                                    self.image_encoder.image_encoder.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, image, text, target=None):\n",
    "        # Image encoding\n",
    "        image_features = self.image_encoder(image)\n",
    "        image_features = image_features[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Text encoding\n",
    "        text_features = self.text_encoder(text)\n",
    "        text_features = text_features[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([image_features, text_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined_features)\n",
    "        \n",
    "        if target is not None:\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            return output, loss\n",
    "        return output\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.text_model_name = 'bert-base-uncased'\n",
    "        self.image_model_name = 'google/vit-base-patch16-224'\n",
    "        self.device = 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load your multimodal model\n",
    "model_path = '/raid/nlp/rajak/Multimodal/UniS-MMC/Path/To/results/models/Food101-best-m3col.pth'\n",
    "model = MMC(args)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
    "model = model.cpu().eval()\n",
    "\n",
    "# Define input resolutions for both modalities\n",
    "image_res = (3, 224, 224)\n",
    "text_res = 25\n",
    "\n",
    "# Custom input constructor for multimodal input\n",
    "def input_constructor(input_res):\n",
    "    image_input = torch.ones((1, *image_res), dtype=torch.float32)\n",
    "    \n",
    "    # Create tensors for text input\n",
    "    text = \"Sample text for tokenization\"\n",
    "    tokenizer = model.text_encoder.get_tokenizer()\n",
    "    encoded_input = tokenizer(text, return_tensors=\"pt\", max_length=text_res, padding='max_length', truncation=True)\n",
    "    \n",
    "    input_ids = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    token_type_ids = encoded_input.get('token_type_ids', torch.zeros_like(input_ids))\n",
    "    \n",
    "    text_input = (input_ids, token_type_ids, attention_mask)\n",
    "    \n",
    "    # Create a dummy target tensor\n",
    "    target = torch.randint(0, 2, (1,), dtype=torch.long)  # Assuming binary classification\n",
    "    \n",
    "    return {'image': image_input, 'text': text_input, 'target': target}\n",
    "\n",
    "# Calculate FLOPs\n",
    "try:\n",
    "    flops, params = get_model_complexity_info(\n",
    "        model, \n",
    "        (image_res, text_res),\n",
    "        input_constructor=input_constructor,\n",
    "        as_strings=True, \n",
    "        print_per_layer_stat=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "    print(f\"Params: {params}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
